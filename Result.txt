Chapter 4: Results and Discussion
4.1 Anomaly Detection Results
4.1.1 Data Processing and Feature Engineering
Before applying clustering algorithms, we developed a robust data processing pipeline to handle the large volume of power quality measurements collected from our sensor networks across Cebu. Our original dataset contained over 3 million raw data points from multiple locations. This section describes the methodology used to transform this large dataset into a representative sample suitable for clustering and classification.

Data Acquisition and Preprocessing
Raw data was collected from filtered_data directory containing CSV files from 6 distinct locations across Cebu's power distribution network. Each location's data was tagged with its geographical identifier to enable location-based analysis later in the process.
Key preprocessing steps included:
Linear interpolation of missing values within each location's data
Median-based imputation for any remaining missing critical values
Validation of timestamp continuity and critical parameter presence
Removal of any rows with persistent missing values in critical columns


Feature Engineering
We engineered a comprehensive set of 10 features to capture both raw measurements and derived indicators of power quality:


Feature Name
Type
Description
Formula
voltage
Raw
Supply voltage (V)
â€“
current
Raw
Current draw measurement (A)
â€“
frequency
Raw
AC frequency measurement (Hz)
â€“
power
Raw
Active power consumption (W)
â€“
powerFactor
Raw
Power factor ratio (dimensionless)
â€“
voltage_deviation
Derived
Relative deviation from nominal voltage
(voltage âˆ’ 230.0) / 230.0
frequency_deviation
Derived
Relative deviation from nominal frequency
(frequency âˆ’ 60.0) / 60.0
pf_deviation
Derived
Deviation from ideal power factor
powerFactor âˆ’ 1.0
power_voltage_ratio
Derived
Power normalized by voltage
power / (voltage + 0.1)
current_voltage_ratio
Derived
Current normalized by voltage
current / (voltage + 0.1)

Table 4.1: Raw and Derived Features Used in Clustering and Classification
These derived features were specifically designed to highlight deviations from nominal values and capture relationships between electrical parameters that signal anomalous conditions.

Strategic Data Reduction
To create a manageable yet representative dataset, we implemented a stratified sampling approach that preserved the diversity and distribution of anomalies:
Anomaly-Based Stratification: Data was categorized into:
Feature Name
Description
voltage_deviation
Deviation of voltage from nominal 230V
frequency_deviation
Deviation of frequency from ideal 60Hz
pf_deviation
Difference between power factor and 1.0
severe_voltage_dip
Voltage < 195V
moderate_voltage_dip
Voltage between 195â€“207V
mild_voltage_dip
Voltage between 207â€“217.4V
mild_surge
Voltage between 242.6â€“248V
moderate_surge
Voltage between 248â€“253V
severe_surge
Voltage > 253V
severe_pf_issue
Power Factor < 0.5
moderate_pf_issue
Power Factor 0.5â€“0.7
mild_pf_issue
Power Factor 0.7â€“0.792
freq_low
Frequency < 59.2 Hz
freq_high
Frequency > 60.8 Hz
high_current
Current > 10 A
very_high_current
Current > 20 A
high_power
Power > 1000 W
very_high_power
Power > 3000 W
transient_flag
Voltage dip with valid frequency (indicates transient dip)
surge_flag
Voltage surge with valid frequency
pf_issue_flag
Power factor below 0.75
power_voltage_ratio
Power divided by voltage
current_voltage_ratio
Current divided by voltage

Table 4.2: Threshold-Based Features for Feature Engineering and Sampling
Normalization
Before applying clustering algorithms, features were normalized using StandardScaler to ensure that parameters with different units and scales would contribute equally to the clustering process.
This comprehensive data processing pipeline ensured that our analysis would be based on a dataset that accurately represented the diversity of power quality conditions across Cebu's distribution network while remaining computationally feasible.

4.1.2 Data Reduction Strategy and Results
To manage computational efficiency and enable meaningful anomaly clustering, a multi-stage data reduction strategy was employed. This process reduced the original 3,721,330 raw sensor readings into a representative dataset of 20,000 samples, while preserving diversity in anomaly types, locations, and voltage conditions.
1. Balanced Anomaly Targeting: An ideal target distribution was defined to include 20% transient anomalies, 20% surge anomalies, 30% power factor (PF) issues, and 30% random data. This ensured each anomaly type was adequately represented in the dataset used for clustering and subsequent classification.
2. Location-Based Stratification: To maintain spatial diversity and ensure fair representation of monitoring sites across Cebuâ€™s power grid, samples were proportionally drawn from each location. A minimum threshold of 50 samples per location was enforced.
3. Voltage Range Preservation: Within each location, voltage values were binned to ensure that the full spectrum of voltage conditionsâ€”including dips and surgesâ€”was preserved in the final dataset. This maintained the datasetâ€™s ability to reflect realistic operational variations.
Anomaly Type
Sample Count
Percentage
Transient anomalies
4,065
20.3%
Surge anomalies
280
1.4%
Power factor issues
9,188
45.9%
Random data
7,875
39.4%

Table 4.3: Final Distribution of Anomaly Types. Note: The percentages in Table 1 sum to more than 100% because some samples may exhibit multiple anomaly types simultaneously.

Location
Sample Count
Tugas
7,396
Lorega
2,441
Naga
1,909
Kodia
1,888
Busay
1,852
Bato
1,204
Tinago
811
SanFernando
792
Poblacion
709
Pusok
593
Talamban
350
Ticad
55

Table 4.4: Sample Distribution by Monitoring Location

4.1.3 Clustering Performance Evaluation
In this study, five clustering algorithms were evaluated for their effectiveness in identifying power quality anomalies in sensor readings: K-Means, Spectral Clustering, Agglomerative Clustering, Ward Hierarchical Clustering, and Gaussian Mixture Model. These algorithms were selected based on their diverse approaches to cluster formation and their suitability for identifying distinct patterns in multidimensional power quality data.

Clustering Algorithm Performance Comparison
The optimal internal validation metrics obtained for each clustering algorithm applied to the sensor dataset are summarized in Table 4.5.
Algorithm
Silhouette Score
Davies-Bouldin Index
Calinski-Harabasz Index
Number of Clusters
KMeans
0.5321
0.8690
17239.31
15
SpectralClustering
-0.0949
1.9033
37.67
4
AgglomerativeClustering
0.5066
0.8446
15943.21
12
WardHierarchical
0.5066
0.8446
15943.21
12
GaussianMixture
0.4806
0.9706
13172.41
6

Table 4.5: Performance Metrics for Clustering Model Selection
These metrics provide different perspectives on clustering quality:
Silhouette Score measures how similar an object is to its own cluster compared to other clusters. The range is from -1 to 1, where higher values indicate better-defined clusters. KMeans achieved the highest score (0.5321), indicating well-separated and cohesive clusters.
Davies-Bouldin Index evaluates intra-cluster similarity and inter-cluster differences, with lower values indicating better clustering. AgglomerativeClustering and WardHierarchical performed best (0.8446), closely followed by KMeans (0.8690).
Calinski-Harabasz Index (Variance Ratio Criterion) measures the ratio of between-cluster dispersion to within-cluster dispersion, with higher values indicating better-defined clusters. KMeans substantially outperformed other algorithms (17239.31).

Selection of Optimal Clustering Model
KMeans was selected as the optimal clustering algorithm for the following reasons:

Superior performance on key metrics: KMeans achieved the highest Silhouette Score (0.5321) and Calinski-Harabasz Index (17239.31), indicating well-separated clusters with high cohesion.
Appropriate cluster granularity: With 15 clusters, KMeans provided sufficient granularity to distinguish between different types of power quality anomalies while avoiding over-segmentation.
Computational efficiency: KMeans completed in significantly less time than hierarchical and spectral methods, making it more practical for large-scale deployment.
Interpretability: The centroids generated by KMeans provided clear, interpretable patterns that could be directly mapped to known power quality anomalies.
While AgglomerativeClustering showed a slightly better Davies-Bouldin Index, the overall pattern of metrics favored KMeans as the most balanced performer across all evaluation criteria.
Spectral Clustering performed poorly in this application, likely due to the complex structure of the power quality data that doesn't align well with the manifold assumptions of spectral methods. The negative Silhouette Score (-0.0949) suggests that many data points might have been assigned to inappropriate clusters.
The optimal number of clusters (k=15) for KMeans was determined through comprehensive evaluation of metrics for values of k ranging from 2 to 15, with particular focus on the Silhouette Score, which showed continuous improvement up to k=15, indicating meaningful cluster separation even at higher granularity.
In the next section, we will examine how these clusters were mapped to specific anomaly types and analyze their distribution across different locations in Cebu.

Clustering Metrics Across Varying Cluster Counts
To determine the optimal number of clusters and assess how each algorithm responds to varying values of k, clustering performance metrics were computed across a range of k=2 to k=15. Figure 2 illustrates the trend of Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index for each algorithm:

Figure 4.1. Clustering Metric Comparison across Varying Cluster Counts
Silhouette Score trends reveal that KMeans consistently improved as k increased, peaking at k=15, further supporting its selection. Agglomerative and Ward Hierarchical Clustering also demonstrated strong cohesion, while Spectral Clustering's performance declined at higher values of k, even becoming unstable.


Davies-Bouldin Index showed that lower values were generally achieved by Agglomerative and Ward Clustering, especially in the range k=10 to k=12, suggesting compact clusters with good separation. KMeans also performed well and maintained stability.


Calinski-Harabasz Index indicated that KMeans formed the most compact and well-separated clusters overall, with a clear maximum at k=15. Agglomerative and Ward methods showed competitive performance but peaked slightly earlier.


This comparative analysis across varying cluster counts provides robust support for choosing KMeans with k=15 as the optimal configuration. It not only achieved the best individual metric values but also demonstrated consistent improvement and stability across metrics.

PCA Visualization of Clustering Results
To visualize the high-dimensional clustering results, Principal Component Analysis (PCA) was applied to reduce the feature space to two dimensions while preserving maximum variance. Figure 1 shows the PCA visualization of the KMeans clustering results:

Figure 4.2: PCA Visualization of KMeans Clustering Results on Sensor Feature Space

PCA Visualization of KMeans Clustering
The PCA projection demonstrates clear separation between most clusters, particularly those representing distinct anomaly types. The first two principal components captured approximately 73% of the total variance in the dataset, indicating that the visualization provides a reasonable representation of the cluster distribution.

4.1.4 Anomaly Classification Results
Following the identification of optimal clustering parameters, each cluster was analyzed to determine its electrical characteristics and the corresponding anomaly type. Table 4.2 provides a detailed classification of power quality clusters, outlining the specific numerical thresholds for key electrical parameters. This classification framework forms the foundation for anomaly identification and supports the supervised learning process by assigning labels to distinct operational and anomaly states.
Cluster
Load Type
Voltage Behavior
Power Factor
Key Observation / Anomaly
Label
0
Light Load (<2A, <400W)
Nominal with peaks >242.6V
Good (0.75-0.85)
Voltage surge under light load
LightLoad_VoltageSurge
1
No Load (Idle) (<0.01A)
Elevated (233-246V), >242.6V
N/A (<0.1)
Overvoltage during idle state
Idle_Overvoltage
2
Light Load (<2A, <400W)
Undervoltage (<217.4V, min 214V)
Moderate (0.7-0.8)
Undervoltage during light load
LightLoad_Undervoltage
3
Very High Load (>20A, >5000W)
Severe swings (212-249V, Â±7.8%)
Excellent (>0.98)
High load with voltage instability
HighLoad_VoltageInstability
4
High Load (10-16A, 2400-3200W)
Extreme dips (<155V), Â±21% variation
Good (>0.9)
Severe transients during high load
HighLoad_SevereTransients
5
No Load (Idle) (<0.01A)
Sustained undervoltage (203-224V)
Very Low (<0.3)
Undervoltage at idle, systemic issue
Idle_Undervoltage
6
Moderate Load (0.6-1.0A, 80-120W)
Mild fluctuation (221-245V)
Very Poor (0.42-0.56)
Reactive load with poor PF
LowPF_ReactiveLoad
7
Low-Moderate (0.3-0.4A, 35-40W)
Occasional surges (225-245V)
Moderate (0.50-0.55)
Minor surges with PF issues
ModeratePF_MinorSurge
8
High Load (12-16A, 2300-3900W)
Wide variation (210-245V, Ïƒ=12.8V)
Moderate (0.7-0.85)
Mixed voltage anomalies at high load
HighLoad_MixedAnomalies
9
Light Load (0.2-0.95A, <100W)
Sustained undervoltage (201-222V)
Poor (0.40-0.45)
Combined low PF and undervoltage
LightLoad_Undervoltage_LowPF
10
Light Load (0.5-2.0A, <250W)
Slightly elevated (230-245V)
Good (0.77-0.80)
Minor voltage surges, efficient load
LightLoad_MinorSurge
11
High Load (5-7A, 1200-1800W)
Stable (225-246V)
Excellent (0.90-0.95)
Ideal high load operation
HighLoad_Optimal
12
No Load (Idle) (<0.01A)
Very stable (235-241V)
N/A (<0.1)
Baseline idle condition
Idle_Stable
13
High Load (5-6A, 950-1350W)
Stable (221-245V)
Near Unity (0.96-0.98)
Excellent operation at high load
HighLoad_Excellent
14
Very High Load (>20A, 4000-5400W)
Stable (235-248V)
Near Unity (0.98-0.99)
Peak load with excellent power quality
PeakLoad_Excellent

Table 4.2: Power Quality Cluster Classification with Numerical Thresholds
This table categorizes each cluster based on combinations of load conditions, voltage behavior, and power factor. The numerical thresholds defined for each cluster enable precise identification of power quality issues, ranging from undervoltage and overvoltage to power factor imbalances and load intensity anomalies. The resulting classification not only enhances automated anomaly detection but also aids in the interpretability of power quality conditions, providing clear guidelines for addressing various operational states.

Visualization of Clustered Electrical Parameters
To validate the separation between identified clusters and assess their alignment with predefined anomaly thresholds, key electrical parametersâ€”voltage, power factor, current, and powerâ€”were visualized across all cluster labels. The following figures illustrate the distribution of these parameters, highlighting operational conditions such as undervoltage, overvoltage, reactive power issues, and load intensity. These visualizations support the interpretability of clustering results and provide a basis for assigning anomaly-type labels in subsequent supervised classification.

Figure 4.3: Cluster Size Distribution Across Identified Operational States
This figure shows the number of data points assigned to each cluster, indicating the frequency of occurrence for different operational and anomaly types. Larger clusters typically represent steady-state or nominal conditions, while smaller clusters often capture rare or extreme power quality issues. Understanding cluster sizes is critical for evaluating anomaly prevalence and ensuring appropriate handling of class imbalance in downstream supervised models.


Figure 4.4: Voltage Distribution Across Clusters
This figure illustrates the voltage distribution within each identified cluster. Dashed lines at 217.4â€¯V and 242.6â€¯V represent the defined anomaly thresholds, highlighting clusters with undervoltage and overvoltage conditions.

Figure 4.5: Power Factor Distribution Across Clusters
Power factor distribution across clusters is shown with thresholds at 0.5, 0.7, and 0.85. These boundaries categorize power factor quality from poor to excellent and help differentiate operational states with reactive power issues.

Figure 4.6: Current Distribution Across Clusters
The current distribution highlights load intensity within each cluster. Clusters are distinguishable based on typical current draw, aiding in classifying operations from idle to very high load.


Figure 4.7: Power Consumption Patterns Across Clusters
This figure presents the active power consumption range for each cluster. It reflects the energy usage characteristics and supports the identification of light, moderate, and heavy load scenarios.

4.1.5 Supervised Classification Accuracy Results
Following the unsupervised clustering and anomaly type labeling process described in the previous sections, supervised classification models were trained to recognize and categorize these operational states in new data. This approach transforms the initial clustering-based anomaly detection into a deployable classification system capable of real-time anomaly identification and categorization.

Classification Models Implemented
Six different classification algorithms were evaluated, each chosen for their distinct strengths in handling power quality data patterns:
Decision Tree: Selected for interpretability and ability to capture threshold-based decision boundaries
Logistic Regression: Provides a baseline linear classifier with low computational requirements
Random Forest: Ensemble method that combines multiple decision trees to reduce overfitting
Support Vector Machine (SVM): Effective for finding optimal separating hyperplanes in high-dimensional feature spaces
Gradient Boosting Machine (GBM): Sequential ensemble method that builds trees to correct errors of previous ones
Multi-Layer Perceptron (MLP): Neural network approach to capture complex non-linear relationships
The models were trained using the 10 features described in Section 3.6.2 (5 raw electrical parameters and 5 derived features), with the target variable being the anomaly type derived from the clustering process. A standard 70-30 train-test split was implemented to ensure robust evaluation.

Feature Scaling Implementation
Not all models require feature scaling for optimal performance. Based on each algorithm's sensitivity to feature scales:


Model Type
Feature Scaling
Reasoning
SVM
 Scaled
Distance-based algorithm sensitive to feature magnitudes
MLP
 Scaled
Neural networks converge faster with normalized inputs
Logistic Regression
 Scaled
Gradient descent optimization performs better with normalized features
Decision Tree
 Unscaled
Split decisions based on thresholds, not affected by scale
Random Forest
 Unscaled
Ensemble of trees inherits scale invariance property
GBM
 Unscaled
Tree-based algorithm naturally handles different scales

Table 4.6: Feature Scaling Implementation
This selective scaling approach optimizes each algorithm's natural strengths, as distance-based models and neural networks benefit from normalized feature ranges, while tree-based methods are naturally invariant to feature scaling.

Performance Results
The classification models demonstrated high accuracy in identifying the 15 distinct operational states, as summarized in Table 4.7:
Model
Accuracy (%)
Training Time (s)
GBM
99.73
40.33
Random Forest
99.73
1.52
Decision Tree
99.67
0.07
MLP
99.58
9.10
Logistic Regression
99.25
0.98
SVM
99.23
3.17

Table 4.7: Performance Results
The unscaled tree-based models perform slightly better than the scaled models, which supports your selective scaling approach. Random Forest offers the best balance of accuracy (99.73%) and speed (1.52s) compared to GBM's similar accuracy but much longer training time (40.33s).
While these models demonstrate excellent classification performance, understanding which features drive their predictions requires explainability techniques, which will be explored in the XAI implementation section.

4.2 XAI Integration (SHAP Analysis)
This section presents the results of integrating Explainable AI (XAI) techniques with our anomaly detection model for power quality monitoring. We focus on SHAP (SHapley Additive exPlanations) to provide transparent explanations for anomaly classifications, implementing the methodology described in Section 3.16.

4.2.1 Evaluation of XAI Metrics
To determine the most suitable XAI approach, we evaluated three SHAP explainers using the evaluation metrics described in Section 3.17: consistency, stability, and compacity. Each explainer offers different advantages in terms of computational efficiency and explanation quality.
Consistency Metric Results
The consistency metric examines whether different explainers produce similar feature importance rankings. Table 4.3 presents the pairwise consistency scores between explainers:
Explainer Pair
Correlation Score
TreeExplainer vs KernelExplainer
0.2414
TreeExplainer vs SamplingExplainer
0.2414
KernelExplainer vs SamplingExplainer
0.2144

Table 4.8: Pairwise Consistency Scores Between SHAP Explainers

The moderate correlation scores (approximately 0.24) indicate some agreement between explainers, but also suggest that different explainers emphasize different aspects of the model's decision-making process. The highest consistency was observed between TreeExplainer and the other two methods.

Stability Metric Results
The stability metric evaluates how robust the explanations are when subjected to minor variations in the input data. A higher stability score indicates more reliable explanations across different data samples.

Explainer
Stability Score
TreeExplainer
1.0000
KernelExplainer
1.0000
SamplingExplainer
1.0000

Table 4.9: Stability Scores for SHAP Explainers
All three explainers TreeExplainer, KernelExplainer, and SamplingExplainer achieved perfect stability scores (1.0000). This indicates that their feature importance explanations remained highly consistent even when the input data was varied across different random subsets. These results support the reliability and robustness of the explainers when applied to real-world anomaly detection in energy data.
Compacity Metric Results
The compacity metric assesses how many features are needed to provide a sufficiently accurate explanation. For this analysis, we set a threshold of 90% explanation quality and determined the minimum number of features required to reach this threshold.
Explainer
Features Required for 90% Explanation Quality
Total Features
TreeExplainer
8
10
KernelExplainer
8
10
SamplingExplainer
8
10

Table 4.10: Compacity Analysis for SHAP Explainers
All three explainers demonstrated the same level of compacity, requiring 8 out of 10 features to achieve 90% explanation quality. This suggests that while the majority of features contribute to the model's decisions, there is some redundancy in the feature set.
Selection of Optimal SHAP Explainer
Based on the evaluation metrics, TreeExplainer was selected as the most suitable option for further analysis. This decision was made considering:
High stability score: TreeExplainer achieved a perfect stability score of 1.0, indicating robust explanations across varied input data
Better consistency: TreeExplainer showed higher consistency with other explainers
Computational efficiency: TreeExplainer is specifically optimized for tree-based models like Random Forest, providing faster explanation generation compared to model-agnostic alternatives
Equal compacity: All explainers showed identical compacity metrics

4.2.2 Global Feature Importance
Global feature importance was analyzed using TreeExplainer. The results show that voltage, power, and power factor were consistently the most influential features in anomaly classification. SHAP summary plots highlight distinct importance patterns across anomaly types, indicating that the model relies on different features depending on the nature of the anomaly.

Figure 4.7: Global Feature Importance Across All Anomaly Types

The analysis revealed the following top five most influential features based on global SHAP values:
Voltage: 0.0278
Voltage Deviation: 0.0270
Power Factor Deviation: 0.0249
Power Factor: 0.0239
Frequency: 0.0194


These results demonstrate that voltage-related features contribute the most to anomaly classification, followed by power factor and frequency attributes. The dominance of these parameters is consistent with established power quality theory, where voltage fluctuations and power factor irregularities are recognized as key indicators of disturbances in electrical distribution systems.

4.2.3 Anomaly-Specific Feature Importance
To better interpret the patterns in SHAP-derived feature importance, the fifteen clustered anomaly types were categorized into two overarching groups based on their severity and operational implications: True Anomalies and Operational Anomalies.

Type
Description
Example Anomaly Labels
ðŸ”´ True Anomalies
Violations of operational thresholds accompanied by irregular patterns
HighLoad_MixedAnomalies
HighLoad_SevereTransients
HighLoad_VoltageInstability
Idle_Overvoltage
Idle_Undervoltage
LightLoad_MinorSurge, LightLoad_Undervoltage
LightLoad_Undervoltage_LowPF
LightLoad_VoltageSurge
LowPF_ReactiveLoad
ModeratePF_MinorSurge
ðŸŸ¡ Operational Anomalies
Technically outside standard ranges but generally stable and efficient
HighLoad_Excellent
HighLoad_Optimal
Idle_Stable
PeakLoad_Excellent

Table 4.11: Power Quality Anomaly Types Categorization

Explanation of Categorization
True Anomalies (11 Types)
These instances exhibit significant deviations from acceptable operational thresholds and typically indicate the presence of critical power quality issues. They are characterized by voltage irregularities, power factor deviations, or a combination of both.
Voltage-related anomalies:
HighLoad_SevereTransients â€“ Severe voltage fluctuations during periods of high load.
HighLoad_VoltageInstability â€“ Voltage instability occurring under heavy power demand.
Idle_Overvoltage â€“ Abnormally high voltage during minimal load conditions.
Idle_Undervoltage â€“ Voltage drop during periods of low or idle load.
LightLoad_MinorSurge â€“ Minor voltage surges under light load conditions.
LightLoad_Undervoltage â€“ Undervoltage occurrences during light load.
LightLoad_VoltageSurge â€“ Sudden voltage spikes under light loading conditions.
Power factor-related anomalies:
LightLoad_Undervoltage_LowPF â€“ Concurrent undervoltage and poor power factor.
LowPF_ReactiveLoad â€“ Extremely low power factor due to a highly reactive load.
ModeratePF_MinorSurge â€“ Moderate power factor deviation coupled with voltage fluctuations.
Mixed anomaly:
HighLoad_MixedAnomalies â€“ Simultaneous occurrence of multiple abnormal behaviors, such as voltage instability and low power factor.

Operational Anomalies (4 Types)
These cases fall marginally outside defined threshold limits but are considered operationally stable or efficient. They do not require immediate corrective actions and often reflect optimal system performance under varying load conditions.
Efficient operation:
HighLoad_Excellent â€“ High demand with excellent power quality metrics.
HighLoad_Optimal â€“ Efficient system operation under high load without signs of degradation.
PeakLoad_Excellent â€“ Excellent performance even at maximum power consumption.
Stable idle state:
Idle_Stable â€“ Normal system stability during periods of minimal power usage.

4.2.4 Anomaly-Specific Global Feature Importance
This section presents the SHAP-based global feature importance analysis for each of the 15 identified anomaly types. The visualizations show the top features influencing the modelâ€™s classification decisions within each cluster. These insights help validate the anomaly definitions and guide future diagnostics or mitigation strategies.




Figure 4.8: Feature Importance â€“ HighLoad_SevereTransients





Figure 4.9: Feature Importance â€“ HighLoad_VoltageInstability





Figure 4.10: Feature Importance â€“ Idle_Overvoltage


Figure 4.11: Feature Importance â€“ Idle_Undervoltage





Figure 4.12: Feature Importance â€“LightLoad_MinorSurge



Figure 4.13: Feature Importance â€“LightLoad_Undervoltage


Figure 4.14: Feature Importance â€“LightLoad_VoltageSurge



Figure 4.15: Feature Importance â€“LightLoad_Undervoltage_LowPF


Figure 4.16: Feature Importance â€“LowPF_ReactiveLoad


Figure 4.17: Feature Importance â€“ModeratePF_MinorSurge


Figure 4.18: Feature Importance â€“Highload_MixedAnomalies


Figure 4.19: Feature Importance â€“Highload_Excellent


Figure 4.20: Feature Importance â€“Highload_Optimal


Figure 4.21: Feature Importance â€“ PeakLoad_Excellent


Figure 4.22: Feature Importance â€“ Idle_Stable



To summarize the insights derived from the visualizations above highlights the top contributing features across all anomaly types, emphasizing the key differences between true anomalies and operational anomalies.
Category
Anomaly Type
Top Feature
2nd Feature
3rd Feature
Key Feature Difference
True Anomalies
HighLoad_SevereTransients
voltage_deviation
current
power
Higher voltage_deviation importance
True Anomalies
LowPF_ReactiveLoad
powerFactor
pf_deviation
power_voltage_ratio
Dominant powerFactor importance
True Anomalies
HighLoad_MixedAnomalies
voltage_deviation
current
power
Mixed importance distribution
True Anomalies
HighLoad_VoltageInstability
current
power
voltage_deviation
Current dominates over voltage features
True Anomalies
Idle_Overvoltage
voltage
voltage_deviation
current
Very strong voltage feature importance
True Anomalies
Idle_Undervoltage
voltage
voltage_deviation
current
High importance of voltage features
True Anomalies
LightLoad_MinorSurge
frequency_deviation
frequency
power_voltage_ratio
Uniquely frequency-dominated
True Anomalies
LightLoad_Undervoltage
voltage
voltage_deviation
pf_deviation
Dominated by absolute voltage
True Anomalies
LightLoad_Undervoltage_LowPF
powerFactor
pf_deviation
voltage_deviation
Combined PF and voltage importance
True Anomalies
LightLoad_VoltageSurge
voltage
voltage_deviation
powerFactor
Very high voltage feature importance
True Anomalies
ModeratePF_MinorSurge
powerFactor
pf_deviation
voltage_deviation
Balanced PF and voltage features
Operational Anomalies
HighLoad_Excellent
current
power
powerFactor
Balanced importance of all three
Operational Anomalies
HighLoad_Optimal
current
power
powerFactor
Load features dominate
Operational Anomalies
Idle_Stable
current
power
current_voltage_ratio
Very low current is most important
Operational Anomalies
PeakLoad_Excellent
power
current
powerFactor
Power feature dominates

Table 4.12: Top Contributing Features by Anomaly Category
This summary confirms that true anomalies typically emphasize deviation-related features (e.g., voltage_deviation, pf_deviation, frequency_deviation), which reflect irregular or unstable behavior. In contrast, operational anomalies tend to show balanced or load-related importance, aligning with normal or efficient operational characteristics.
4.2.5 Local Explanations for Anomalies
This section demonstrates how SHAP (SHapley Additive exPlanations) provides detailed interpretability for individual power quality anomalies. While global feature importance helps understand the model's behavior across all predictions, local explanations reveal why specific readings are classified as particular anomaly types, offering actionable insights for utility operators.

Case Study: LightLoad_MinorSurge Anomaly from Bato
To illustrate the effectiveness of local explanations, we analyzed a specific sensor reading (index 5252) from Bato that was classified as "LightLoad_MinorSurge" by our model. This anomaly type represents a condition where voltage is slightly elevated under light load conditions.
Sensor Reading Characteristics
The reading exhibited the following electrical parameters:
Voltage: 240.40 V (slightly elevated, but below severe surge threshold)
Current: 0.942 A (confirming light load operation)
Power: 154.90 W (consistent with light load classification)
Power Factor: 0.68 (moderately reduced)
Frequency: 59.80 Hz (slight negative deviation from nominal 60 Hz)
SHAP Analysis Results

Figure 4.23: SHAP waterfall plot explaining classification of reading #5252 as LightLoad_MinorSurge

The SHAP waterfall plot reveals that:
Frequency-related features dominated the classification decision:
frequency_deviation (contribution: 0.1460) - The slight negative deviation (-0.0033) from nominal frequency was the strongest factor
frequency (contribution: 0.1370) - The absolute frequency value of 59.80 Hz was the second most important factor
Secondary contributing factors included:
power_voltage_ratio (0.0968) - Indicating the relationship between power consumption and voltage level
powerFactor (0.0922) - The moderate power factor of 0.68
power (0.0916) - The light load consumption of 154.9W
Voltage measurements were less influential despite this being categorized as a voltage-related anomaly:
voltage (0.0862) and voltage_deviation (0.0830) had lower contributions

Interpretation and Practical Significance
This local explanation reveals an important insight: while the anomaly is categorized as "LightLoad_MinorSurge" (suggesting voltage is the primary concern), the model actually placed greater importance on frequency characteristics when making this specific classification. This nuance would not be apparent from global feature importance alone.
For utility operators in Bato, this explanation suggests:
Monitoring focus: While voltage appears slightly elevated (240.40V), closer attention should be paid to frequency stability at this location, as even small deviations significantly influence anomaly detection.
Maintenance prioritization: The combination of reduced frequency and moderate power factor may indicate potential issues with reactive loads or distribution system resonance rather than a simple voltage regulation problem.
Root cause analysis: The moderate power factor (0.68) combined with the frequency deviation points to potential harmonic issues or inductive loads affecting power quality, rather than upstream voltage regulation problems.

This example demonstrates how SHAP explanations transform "black box" anomaly classifications into actionable insights by identifying which electrical parameters truly drive specific anomaly detections.

Cross-Location Analysis of Local Explanations
To understand how anomaly characteristics vary across Cebu's power distribution network, we conducted similar analyses for representative samples from different locations. These analyses revealed location-specific patterns:
Lorega: Voltage features typically dominated anomaly classifications, suggesting voltage regulation as the primary concern in this area.
Tugas: Power factor deviations were more influential, indicating potential reactive power management issues.
Bato: As shown in our case study, frequency features often had unexpectedly high importance despite anomalies being categorized as voltage-related.
These location-specific insights enable targeted infrastructure improvements and more efficient resource allocation by distribution utilities across Cebu.
Practical Applications of Local Explanations
The local explanations generated through SHAP provide several practical benefits for power quality monitoring:
Verification of classifications: Engineers can verify whether model predictions align with their domain expertise based on the feature contributions.
Targeted maintenance: Resources can be directed to address specific components causing issues, rather than general system maintenance.
Knowledge discovery: Unexpected feature importance patterns (like frequency dominating a voltage anomaly classification) can reveal hidden relationships between power quality parameters.
Training material: These explanations serve as effective training examples for new personnel to understand the complex interactions between electrical parameters.
By combining global feature importance with local explanations, utility operators gain a comprehensive understanding of both system-wide patterns and specific anomaly instances, enabling more effective monitoring and maintenance of Cebu's power distribution network.

