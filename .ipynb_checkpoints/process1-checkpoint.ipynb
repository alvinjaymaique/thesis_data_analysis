{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4567aa4-8937-4629-ac31-d63a4e5c23a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Power Quality Anomaly Analysis ===\n",
      "Started at: 2025-05-01 15:29:15\n",
      "\n",
      "[Step 1/8] Loading anomalous data...\n",
      "Loaded Bato - 131499 rows\n",
      "Loaded Busay - 663099 rows\n",
      "Loaded Kodia - 561491 rows\n",
      "Loaded Lorega - 192232 rows\n",
      "Loaded Naga - 678014 rows\n",
      "Loaded Poblacion - 261193 rows\n",
      "Loaded Pusok - 53154 rows\n",
      "Loaded SanFernando - 277521 rows\n",
      "Loaded Talamban - 32157 rows\n",
      "Loaded Ticad - 6018 rows\n",
      "Loaded Tinago - 273387 rows\n",
      "Loaded Tugas - 591565 rows\n",
      "Total combined data: 3721330 rows\n",
      "Step 1 completed in 6.0 seconds\n",
      "\n",
      "[Step 2/8] Preprocessing data...\n",
      "\n",
      "Preprocessing data...\n",
      "Missing values before: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:57: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after: 0\n",
      "Rows after handling missing values: 3721330\n",
      "Step 2 completed in 58.4 seconds\n",
      "\n",
      "[Step 3/8] Engineering features...\n",
      "\n",
      "Engineering features...\n",
      "Step 3 completed in 1.2 seconds\n",
      "\n",
      "[Step 4/8] Reducing data...\n",
      "\n",
      "Reducing data...\n",
      "Processing 3,721,330 rows to reduce to 20,000 samples\n",
      "Found 95912 transient anomalies\n",
      "Found 280 surge anomalies\n",
      "Found 2961301 power factor issues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_18528\\624310799.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced data from 3,721,330 to 20,000 rows\n",
      "\n",
      "Anomaly type distribution in sampled data:\n",
      " - Transient anomalies: 4065 (20.3%)\n",
      " - Surge anomalies: 280 (1.4%)\n",
      " - PF issues: 9188 (45.9%)\n",
      " - Normal data: 7875 (39.4%)\n",
      "\n",
      "Samples per location:\n",
      " - Tugas: 7396 rows\n",
      " - Lorega: 2441 rows\n",
      " - Naga: 1909 rows\n",
      " - Kodia: 1888 rows\n",
      " - Busay: 1852 rows\n",
      " - Bato: 1204 rows\n",
      " - Tinago: 811 rows\n",
      " - SanFernando: 792 rows\n",
      " - Poblacion: 709 rows\n",
      " - Pusok: 593 rows\n",
      " - Talamban: 350 rows\n",
      " - Ticad: 55 rows\n",
      "Step 4 completed in 4.7 seconds\n",
      "\n",
      "[Step 5/8] Normalizing data...\n",
      "\n",
      "Normalizing data...\n",
      "Data normalized using standard scaling\n",
      "Step 5 completed in 0.0 seconds\n",
      "\n",
      "[Step 7/8] Performing clustering analysis...\n",
      "\n",
      "Performing clustering analysis...\n",
      "Clustering 20,000 data points across 10 features\n",
      "\n",
      "Finding optimal clusters for KMeans...\n",
      "  k=2: silhouette=0.2949\n",
      "  k=3: silhouette=0.4141\n",
      "  k=4: silhouette=0.4455\n",
      "  k=5: silhouette=0.4619\n",
      "  k=6: silhouette=0.5175\n",
      "  k=7: silhouette=0.4868\n",
      "  k=8: silhouette=0.5227\n",
      "  k=9: silhouette=0.5182\n",
      "  k=10: silhouette=0.5042\n",
      "  Best k for KMeans: 8 (score=0.5227)\n",
      "\n",
      "Finding optimal clusters for SpectralClustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "TQDM_AVAILABLE = True\n",
    "\n",
    "# 1. Load all filtered anomalous data files\n",
    "def load_anomalous_data(data_dir='filtered_data'):\n",
    "    \"\"\"Load and combine all filtered anomalous data files, adding location info.\"\"\"\n",
    "    all_files = glob.glob(f'{data_dir}/*_filtered.csv')\n",
    "    data_frames = []\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            location = os.path.basename(file_path).replace('_filtered.csv', '')\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['location'] = location\n",
    "            data_frames.append(df)\n",
    "            print(f\"Loaded {location} - {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    print(f\"Total combined data: {len(combined_df)} rows\")\n",
    "    return combined_df\n",
    "\n",
    "# 2. Preprocess and clean data\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Handle missing values, standardize columns, and prepare data.\"\"\"\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime if present\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Ensure critical columns exist\n",
    "    critical_cols = ['voltage', 'current', 'frequency', 'power', 'powerFactor']\n",
    "    for col in critical_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Critical column '{col}' missing from data\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"Missing values before: {df[critical_cols].isna().sum().sum()}\")\n",
    "    \n",
    "    # Group by location and interpolate within each location\n",
    "    df = df.groupby('location').apply(lambda x: x.interpolate(method='linear'))\n",
    "    \n",
    "    # If any remaining NaNs, fill with median values by location\n",
    "    for loc in df['location'].unique():\n",
    "        loc_mask = df['location'] == loc\n",
    "        for col in critical_cols:\n",
    "            median_val = df.loc[loc_mask, col].median()\n",
    "            df.loc[loc_mask, col] = df.loc[loc_mask, col].fillna(median_val)\n",
    "    \n",
    "    # Drop any rows that still have NaN in critical columns\n",
    "    df = df.dropna(subset=critical_cols)\n",
    "    print(f\"Missing values after: {df[critical_cols].isna().sum().sum()}\")\n",
    "    print(f\"Rows after handling missing values: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. Feature Engineering\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create detailed features to help distinguish anomaly types.\"\"\"\n",
    "    print(\"\\nEngineering features...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define nominal/ideal values for Philippines power systems\n",
    "    NOMINAL_VOLTAGE = 230.0    # Nominal voltage for Philippines (V)\n",
    "    IDEAL_FREQUENCY = 60.0     # Ideal frequency (Hz)\n",
    "    IDEAL_POWER_FACTOR = 1.0   # Ideal power factor\n",
    "    \n",
    "    # Calculate deviations from nominal values\n",
    "    df['voltage_deviation'] = (df['voltage'] - NOMINAL_VOLTAGE) / NOMINAL_VOLTAGE\n",
    "    df['frequency_deviation'] = (df['frequency'] - IDEAL_FREQUENCY) / IDEAL_FREQUENCY\n",
    "    df['pf_deviation'] = df['powerFactor'] - IDEAL_POWER_FACTOR\n",
    "    \n",
    "    # DETAILED VOLTAGE ANOMALY CATEGORIES\n",
    "    df['severe_voltage_dip'] = ((df['voltage'] < 195.0)).astype(int)\n",
    "    df['moderate_voltage_dip'] = ((df['voltage'] >= 195.0) & (df['voltage'] < 207.0)).astype(int)\n",
    "    df['mild_voltage_dip'] = ((df['voltage'] >= 207.0) & (df['voltage'] < 217.4)).astype(int)\n",
    "    \n",
    "    df['mild_surge'] = ((df['voltage'] > 242.6) & (df['voltage'] <= 248.0)).astype(int)\n",
    "    df['moderate_surge'] = ((df['voltage'] > 248.0) & (df['voltage'] <= 253.0)).astype(int)\n",
    "    df['severe_surge'] = ((df['voltage'] > 253.0)).astype(int)\n",
    "    \n",
    "    # DETAILED POWER FACTOR ANOMALY CATEGORIES\n",
    "    df['severe_pf_issue'] = ((df['powerFactor'] < 0.5)).astype(int)\n",
    "    df['moderate_pf_issue'] = ((df['powerFactor'] >= 0.5) & (df['powerFactor'] < 0.7)).astype(int)\n",
    "    df['mild_pf_issue'] = ((df['powerFactor'] >= 0.7) & (df['powerFactor'] < 0.792)).astype(int)\n",
    "    \n",
    "    # FREQUENCY ANOMALIES\n",
    "    df['freq_low'] = ((df['frequency'] < 59.2)).astype(int)\n",
    "    df['freq_high'] = ((df['frequency'] > 60.8)).astype(int)\n",
    "    \n",
    "    # LOAD AND POWER ANOMALIES\n",
    "    df['high_current'] = (df['current'] > 10.0).astype(int)\n",
    "    df['very_high_current'] = (df['current'] > 20.0).astype(int)\n",
    "    df['high_power'] = (df['power'] > 1000.0).astype(int)\n",
    "    df['very_high_power'] = (df['power'] > 3000.0).astype(int)\n",
    "    \n",
    "    # Original basic flags (for compatibility)\n",
    "    df['transient_flag'] = ((df['voltage'] < 207.0) & (df['frequency'] > 59.0) & \n",
    "                           (df['frequency'] < 61.0)).astype(int)\n",
    "    df['surge_flag'] = ((df['voltage'] > 248.0) & (df['frequency'] > 59.0) & \n",
    "                       (df['frequency'] < 61.0)).astype(int)\n",
    "    df['pf_issue_flag'] = (df['powerFactor'] < 0.75).astype(int)\n",
    "    \n",
    "    # Create ratios (useful for detecting unusual load patterns)\n",
    "    df['power_voltage_ratio'] = df['power'] / (df['voltage'] + 0.1)\n",
    "    df['current_voltage_ratio'] = df['current'] / (df['voltage'] + 0.1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Reduce data while preserving meaningful patterns\n",
    "def reduce_data(df, max_samples=20000):\n",
    "    \"\"\"Reduce dataset size while ensuring balanced representation of anomaly types.\"\"\"\n",
    "    print(\"\\nReducing data...\")\n",
    "    original_len = len(df)\n",
    "    print(f\"Processing {original_len:,} rows to reduce to {max_samples:,} samples\")\n",
    "    \n",
    "    if len(df) <= max_samples:\n",
    "        print(f\"Data already small enough ({len(df):,} rows)\")\n",
    "        return df\n",
    "    \n",
    "    # STEP 1: Identify rows by anomaly type\n",
    "    transient_rows = df[df['transient_flag'] == 1]\n",
    "    surge_rows = df[df['surge_flag'] == 1]\n",
    "    pf_issue_rows = df[df['pf_issue_flag'] == 1]\n",
    "    print(f\"Found {len(transient_rows)} transient anomalies\")\n",
    "    print(f\"Found {len(surge_rows)} surge anomalies\")\n",
    "    print(f\"Found {len(pf_issue_rows)} power factor issues\")\n",
    "    \n",
    "    # STEP 2: Calculate samples for each category\n",
    "    # Allocate samples: 20% transients, 20% surges, 30% PF issues, 30% normal data\n",
    "    # If a category has fewer rows than its allocation, take all available and redistribute\n",
    "    transient_target = int(max_samples * 0.2)  # 20% for transients\n",
    "    surge_target = int(max_samples * 0.2)      # 20% for surges\n",
    "    pf_issue_target = int(max_samples * 0.3)   # 30% for PF issues\n",
    "    normal_target = max_samples - transient_target - surge_target - pf_issue_target  # Remaining for normal\n",
    "    \n",
    "    # Adjust if we don't have enough rows of a certain type\n",
    "    transient_sample_size = min(len(transient_rows), transient_target)\n",
    "    remainder = transient_target - transient_sample_size\n",
    "    \n",
    "    surge_target += remainder // 3\n",
    "    pf_issue_target += remainder // 3\n",
    "    normal_target += remainder - (2 * (remainder // 3))\n",
    "    \n",
    "    surge_sample_size = min(len(surge_rows), surge_target)\n",
    "    remainder = surge_target - surge_sample_size\n",
    "    \n",
    "    pf_issue_target += remainder // 2\n",
    "    normal_target += remainder - (remainder // 2)\n",
    "    \n",
    "    pf_issue_sample_size = min(len(pf_issue_rows), pf_issue_target)\n",
    "    normal_target += (pf_issue_target - pf_issue_sample_size)\n",
    "    \n",
    "    # STEP 3: Sample from each category\n",
    "    sampled_transients = transient_rows.sample(transient_sample_size, random_state=42) if transient_sample_size > 0 else pd.DataFrame()\n",
    "    sampled_surges = surge_rows.sample(surge_sample_size, random_state=42) if surge_sample_size > 0 else pd.DataFrame()\n",
    "    sampled_pf_issues = pf_issue_rows.sample(pf_issue_sample_size, random_state=42) if pf_issue_sample_size > 0 else pd.DataFrame()\n",
    "    \n",
    "    # STEP 4: Get normal data (not flagged by any anomaly)\n",
    "    all_anomalies = pd.concat([sampled_transients, sampled_surges, sampled_pf_issues])\n",
    "    normal_data = df[~((df['transient_flag'] == 1) | (df['surge_flag'] == 1) | (df['pf_issue_flag'] == 1))]\n",
    "    \n",
    "    # STEP 5: Create stratified sample of normal data by location\n",
    "    normal_sample = pd.DataFrame()\n",
    "    if len(normal_data) > 0 and normal_target > 0:\n",
    "        for location in normal_data['location'].unique():\n",
    "            loc_data = normal_data[normal_data['location'] == location]\n",
    "            loc_proportion = len(loc_data) / len(normal_data)\n",
    "            loc_sample_size = max(50, int(normal_target * loc_proportion))  # At least 50 from each location\n",
    "            \n",
    "            if len(loc_data) > 0:\n",
    "                # Create voltage bins for stratified sampling\n",
    "                loc_data['voltage_bin'] = pd.qcut(loc_data['voltage'], \n",
    "                                                 q=min(10, len(loc_data)),\n",
    "                                                 labels=False, \n",
    "                                                 duplicates='drop')\n",
    "                \n",
    "                loc_sample = pd.DataFrame()\n",
    "                for bin_id in loc_data['voltage_bin'].unique():\n",
    "                    bin_data = loc_data[loc_data['voltage_bin'] == bin_id]\n",
    "                    bin_sample_size = max(1, int(loc_sample_size * len(bin_data) / len(loc_data)))\n",
    "                    if len(bin_data) > 0:\n",
    "                        bin_sample = bin_data.sample(min(bin_sample_size, len(bin_data)), random_state=42)\n",
    "                        loc_sample = pd.concat([loc_sample, bin_sample])\n",
    "                \n",
    "                normal_sample = pd.concat([normal_sample, loc_sample])\n",
    "    \n",
    "    # If we still need more normal samples, add them randomly\n",
    "    if len(normal_sample) < normal_target and len(normal_data) > 0:\n",
    "        remaining_normal = normal_data[~normal_data.index.isin(normal_sample.index)]\n",
    "        if len(remaining_normal) > 0:\n",
    "            additional_samples = min(normal_target - len(normal_sample), len(remaining_normal))\n",
    "            normal_sample = pd.concat([normal_sample, \n",
    "                                     remaining_normal.sample(additional_samples, random_state=42)])\n",
    "    \n",
    "    # STEP 6: Combine all samples\n",
    "    reduced_df = pd.concat([sampled_transients, sampled_surges, sampled_pf_issues, normal_sample])\n",
    "    \n",
    "    # Ensure we don't exceed max_samples\n",
    "    if len(reduced_df) > max_samples:\n",
    "        reduced_df = reduced_df.sample(max_samples, random_state=42)\n",
    "    \n",
    "    print(f\"Reduced data from {original_len:,} to {len(reduced_df):,} rows\")\n",
    "    print(\"\\nAnomaly type distribution in sampled data:\")\n",
    "    print(f\" - Transient anomalies: {sum(reduced_df['transient_flag'])} ({sum(reduced_df['transient_flag'])/len(reduced_df):.1%})\")\n",
    "    print(f\" - Surge anomalies: {sum(reduced_df['surge_flag'])} ({sum(reduced_df['surge_flag'])/len(reduced_df):.1%})\")\n",
    "    print(f\" - PF issues: {sum(reduced_df['pf_issue_flag'])} ({sum(reduced_df['pf_issue_flag'])/len(reduced_df):.1%})\")\n",
    "    print(f\" - Normal data: {len(reduced_df) - sum(reduced_df['transient_flag'] | reduced_df['surge_flag'] | reduced_df['pf_issue_flag'])} ({(len(reduced_df) - sum(reduced_df['transient_flag'] | reduced_df['surge_flag'] | reduced_df['pf_issue_flag']))/len(reduced_df):.1%})\")\n",
    "    \n",
    "    # Print location distribution\n",
    "    loc_counts = reduced_df['location'].value_counts()\n",
    "    print(\"\\nSamples per location:\")\n",
    "    for loc, count in loc_counts.items():\n",
    "        print(f\" - {loc}: {count} rows\")\n",
    "        \n",
    "    return reduced_df\n",
    "\n",
    "# 5. Normalize/Scale data for clustering\n",
    "def normalize_data(df, scaler_type='standard'):\n",
    "    \"\"\"Normalize numerical features for clustering.\"\"\"\n",
    "    print(\"\\nNormalizing data...\")\n",
    "    \n",
    "    # Select features for clustering\n",
    "    feature_cols = [\n",
    "        'voltage', 'current', 'frequency', 'power', 'powerFactor',\n",
    "        'voltage_deviation', 'frequency_deviation', 'pf_deviation',\n",
    "        'power_voltage_ratio', 'current_voltage_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Select appropriate scaler\n",
    "    if scaler_type.lower() == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type.lower() == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type.lower() == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaler type: {scaler_type}\")\n",
    "    \n",
    "    # Fit scaler and transform the data\n",
    "    features = df[feature_cols].copy()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Create DataFrame with scaled features\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols)\n",
    "    \n",
    "    # Add location and flag columns (not scaled)\n",
    "    for col in ['location', 'transient_flag', 'surge_flag', 'pf_issue_flag']:\n",
    "        if col in df.columns:\n",
    "            scaled_df[col] = df[col].values\n",
    "    \n",
    "    print(f\"Data normalized using {scaler_type} scaling\")\n",
    "    return scaled_df, feature_cols, scaler\n",
    "\n",
    "# 6. Visualize the data\n",
    "def visualize_data(df, scaled_df, feature_cols):\n",
    "    \"\"\"Create visualizations with better performance.\"\"\"\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # 1. Distribution of key parameters\n",
    "    print(\"  Creating parameter distributions plot...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(['voltage', 'current', 'frequency', 'power', 'powerFactor']):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sample_size = min(5000, len(df))\n",
    "        sns.histplot(df[col].sample(sample_size), kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/parameter_distributions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Visualize patterns with PCA (2D projection)\n",
    "    print(\"  Creating PCA projection plots...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    \n",
    "    # Sample for PCA visualization\n",
    "    sample_size = min(5000, len(scaled_df))\n",
    "    sample_indices = np.random.choice(len(scaled_df), sample_size, replace=False)\n",
    "    sampled_df = df.iloc[sample_indices]\n",
    "    sampled_scaled_df = scaled_df.iloc[sample_indices]\n",
    "    \n",
    "    pca_result = pca.fit_transform(sampled_scaled_df[feature_cols])\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Color by location - USING sampled_df\n",
    "    print(\"  - Creating location PCA plot...\")\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for location in sampled_df['location'].unique():\n",
    "        mask = sampled_df['location'] == location  # FIXED: using sampled_df\n",
    "        if sum(mask) > 0:\n",
    "            plt.scatter(pca_result[mask, 0], pca_result[mask, 1], label=location, alpha=0.6, s=10)\n",
    "    plt.title('PCA Projection by Location')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    # Color by transient flag - USING sampled_df\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for flag in [0, 1]:\n",
    "        mask = sampled_df['transient_flag'] == flag  # FIXED: using sampled_df\n",
    "        plt.scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                   label=f'Transient: {\"Yes\" if flag else \"No\"}', \n",
    "                   alpha=0.6, s=10)\n",
    "    plt.title('PCA Projection by Transient Flag')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Color by surge flag - USING sampled_df\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for flag in [0, 1]:\n",
    "        mask = sampled_df['surge_flag'] == flag  # FIXED: using sampled_df\n",
    "        plt.scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                   label=f'Surge: {\"Yes\" if flag else \"No\"}', \n",
    "                   alpha=0.6, s=10)\n",
    "    plt.title('PCA Projection by Voltage Surge Flag')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Color by PF issue flag - USING sampled_df\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for flag in [0, 1]:\n",
    "        mask = sampled_df['pf_issue_flag'] == flag  # FIXED: using sampled_df\n",
    "        plt.scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                   label=f'PF Issue: {\"Yes\" if flag else \"No\"}', \n",
    "                   alpha=0.6, s=10)\n",
    "    plt.title('PCA Projection by Power Factor Issue Flag')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/pca_visualization.png')\n",
    "    plt.close()  # Close the figure\n",
    "    \n",
    "    # 3. Correlation matrix of features\n",
    "    print(\"  Creating correlation matrix...\")\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = df[feature_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix of Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/correlation_matrix.png')\n",
    "    plt.close()  # Close the figure\n",
    "    \n",
    "    print(\"  Visualizations saved to 'plots' directory\")\n",
    "    return pca_result\n",
    "\n",
    "def find_optimal_clusters(X, model_class, max_k=15, random_state=42, **model_params):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters using silhouette method for any clustering model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - model_class: The clustering class to use (e.g., KMeans, AgglomerativeClustering)\n",
    "    - max_k: Maximum number of clusters to try\n",
    "    - random_state: Random seed for reproducibility\n",
    "    - **model_params: Additional parameters to pass to the model\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_k: Optimal number of clusters\n",
    "    - best_score: Best silhouette score achieved\n",
    "    - scores: List of silhouette scores for each k\n",
    "    \"\"\"\n",
    "    print(f\"\\nFinding optimal clusters for {model_class.__name__}...\")\n",
    "    best_k = 2  # minimum 2 clusters\n",
    "    best_score = -1\n",
    "    scores = []\n",
    "    \n",
    "    for k in range(2, max_k+1):\n",
    "        # Create model instance with current k\n",
    "        try:\n",
    "            if model_class == KMeans:\n",
    "                model = model_class(n_clusters=k, random_state=random_state, **model_params)\n",
    "            elif model_class == GaussianMixture:\n",
    "                model = model_class(n_components=k, random_state=random_state, **model_params)\n",
    "            elif model_class == SpectralClustering:\n",
    "                model = model_class(n_clusters=k, random_state=random_state, **model_params)\n",
    "            elif model_class == AgglomerativeClustering:\n",
    "                model = model_class(n_clusters=k, **model_params)\n",
    "            else:\n",
    "                print(f\"  Unsupported model class: {model_class.__name__}\")\n",
    "                return 2, -1, []\n",
    "            \n",
    "            # Fit model and get cluster labels\n",
    "            if hasattr(model, 'fit_predict'):\n",
    "                labels = model.fit_predict(X)\n",
    "            else:\n",
    "                model.fit(X)\n",
    "                labels = model.predict(X)\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            score = silhouette_score(X, labels)\n",
    "            scores.append(score)\n",
    "            print(f\"  k={k}: silhouette={score:.4f}\")\n",
    "            \n",
    "            # Update best score and k if better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_k = k\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with k={k}: {str(e)}\")\n",
    "            scores.append(float('nan'))\n",
    "    \n",
    "    print(f\"  Best k for {model_class.__name__}: {best_k} (score={best_score:.4f})\")\n",
    "    return best_k, best_score, scores\n",
    "\n",
    "# 7. Perform clustering and evaluate\n",
    "def perform_clustering(scaled_df, feature_cols):\n",
    "    \"\"\"Perform clustering using multiple methods and evaluate results.\"\"\"\n",
    "    print(\"\\nPerforming clustering analysis...\")\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    X = scaled_df[feature_cols].values\n",
    "    total_samples = X.shape[0]\n",
    "    print(f\"Clustering {total_samples:,} data points across {len(feature_cols)} features\")\n",
    "    \n",
    "    # Define clustering model classes to try\n",
    "    model_classes = {\n",
    "        'KMeans': KMeans,\n",
    "        'SpectralClustering': SpectralClustering,\n",
    "        'AgglomerativeClustering': AgglomerativeClustering,\n",
    "        'WardHierarchical': AgglomerativeClustering,\n",
    "        'GaussianMixture': GaussianMixture\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Find optimal k for each model type\n",
    "    optimal_ks = {}\n",
    "    for name, model_class in model_classes.items():\n",
    "        if name == 'WardHierarchical':\n",
    "            optimal_ks[name], _, _ = find_optimal_clusters(X, model_class, linkage='ward')\n",
    "        elif name == 'SpectralClustering':\n",
    "            optimal_ks[name], _, _ = find_optimal_clusters(X, model_class, affinity='nearest_neighbors')\n",
    "        else:\n",
    "            optimal_ks[name], _, _ = find_optimal_clusters(X, model_class)\n",
    "    \n",
    "    # Create models with their optimal k\n",
    "    clustering_models = {\n",
    "        'KMeans': KMeans(n_clusters=optimal_ks['KMeans'], random_state=42, verbose=1),\n",
    "        'SpectralClustering': SpectralClustering(n_clusters=optimal_ks['SpectralClustering'], random_state=42, affinity='nearest_neighbors'),\n",
    "        'AgglomerativeClustering': AgglomerativeClustering(n_clusters=optimal_ks['AgglomerativeClustering']),\n",
    "        'WardHierarchical': AgglomerativeClustering(n_clusters=optimal_ks['WardHierarchical'], linkage='ward'),\n",
    "        'GaussianMixture': GaussianMixture(n_components=optimal_ks['GaussianMixture'], random_state=42, verbose=1)\n",
    "    }\n",
    "      # Create models with their optimal k\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Perform clustering and calculate metrics\n",
    "    for name, model in clustering_models.items():\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nRunning {name}... [Started at {datetime.now().strftime('%H:%M:%S')}]\")\n",
    "        \n",
    "        # Fit the model and get labels\n",
    "        if hasattr(model, 'fit_predict'):\n",
    "            print(f\"  - Fitting and predicting with {name}...\")\n",
    "            labels = model.fit_predict(X)\n",
    "        else:\n",
    "            print(f\"  - Fitting {name}...\")\n",
    "            model.fit(X)\n",
    "            print(f\"  - Predicting with {name}...\")\n",
    "            labels = model.predict(X)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        metrics = {}\n",
    "        print(\"  - Calculating evaluation metrics...\")\n",
    "        \n",
    "        try:\n",
    "            print(\"    - Computing Silhouette Score...\")\n",
    "            metrics['silhouette'] = silhouette_score(X, labels)\n",
    "        except Exception as e:\n",
    "            print(f\"    - Silhouette Score failed: {str(e)}\")\n",
    "            metrics['silhouette'] = float('nan')\n",
    "            \n",
    "        try:\n",
    "            print(\"    - Computing Davies-Bouldin Index...\")\n",
    "            metrics['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
    "        except Exception as e:\n",
    "            print(f\"    - Davies-Bouldin Index failed: {str(e)}\")\n",
    "            metrics['davies_bouldin'] = float('nan')\n",
    "            \n",
    "        try:\n",
    "            print(\"    - Computing Calinski-Harabasz Index...\")\n",
    "            metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
    "        except Exception as e:\n",
    "            print(f\"    - Calinski-Harabasz Index failed: {str(e)}\")\n",
    "            metrics['calinski_harabasz'] = float('nan')\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'labels': labels,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Calculate and display time taken\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        time_str = f\"{int(elapsed_time // 60)} mins {int(elapsed_time % 60)} secs\"\n",
    "        \n",
    "        print(f\"  {name} completed in {time_str}\")\n",
    "        print(f\"  Silhouette: {metrics['silhouette']:.4f}\")\n",
    "        print(f\"  Davies-Bouldin: {metrics['davies_bouldin']:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz: {metrics['calinski_harabasz']:.4f}\")\n",
    "    \n",
    "    # Create dataframe of metrics for comparison\n",
    "    metrics_df = pd.DataFrame({\n",
    "        name: {\n",
    "            'Silhouette Score': results[name]['metrics']['silhouette'],\n",
    "            'Davies-Bouldin Index': results[name]['metrics']['davies_bouldin'],\n",
    "            'Calinski-Harabasz Index': results[name]['metrics']['calinski_harabasz']\n",
    "        } for name in results\n",
    "    })\n",
    "    \n",
    "    # Find best model based on silhouette score\n",
    "    best_model = max(results.keys(), key=lambda x: results[x]['metrics']['silhouette'] \n",
    "                     if not np.isnan(results[x]['metrics']['silhouette']) else -np.inf)\n",
    "    \n",
    "    # Add best cluster labels to df\n",
    "    scaled_df['cluster'] = results[best_model]['labels']\n",
    "    \n",
    "    print(f\"\\nBest clustering model: {best_model}\")\n",
    "    print(metrics_df[best_model])\n",
    "    \n",
    "    return results, metrics_df, best_model\n",
    "\n",
    "# 8. Analyze clusters\n",
    "def analyze_clusters(df, scaled_df, results, best_model):\n",
    "    \"\"\"Analyze the clusters to determine what they represent.\"\"\"\n",
    "    print(\"\\nAnalyzing clusters...\")\n",
    "    \n",
    "    cluster_labels = results[best_model]['labels']\n",
    "    df_with_clusters = df.copy()\n",
    "    df_with_clusters['cluster'] = cluster_labels\n",
    "    \n",
    "    # Count samples per cluster\n",
    "    cluster_counts = df_with_clusters['cluster'].value_counts().sort_index()\n",
    "    print(\"Samples per cluster:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"  Cluster {cluster}: {count} samples ({count/len(df_with_clusters):.1%})\")\n",
    "    \n",
    "    # Calculate cluster profiles - INCLUDE ALL DETAILED ANOMALY CATEGORIES\n",
    "    cluster_profiles = df_with_clusters.groupby('cluster')[\n",
    "        ['voltage', 'current', 'frequency', 'power', 'powerFactor', \n",
    "        'transient_flag', 'surge_flag', 'pf_issue_flag',\n",
    "        'severe_voltage_dip', 'moderate_voltage_dip', 'mild_voltage_dip',\n",
    "        'mild_surge', 'moderate_surge', 'severe_surge', \n",
    "        'severe_pf_issue', 'moderate_pf_issue', 'mild_pf_issue',\n",
    "        'freq_low', 'freq_high',\n",
    "        'high_current', 'very_high_current', 'high_power', 'very_high_power',\n",
    "        'voltage_deviation', 'frequency_deviation', 'pf_deviation',  # Add these three lines\n",
    "        'power_voltage_ratio', 'current_voltage_ratio'              # Add derived ratios too\n",
    "        ]\n",
    "    ].mean()\n",
    "    \n",
    "    print(\"\\nCluster profiles:\")\n",
    "    print(cluster_profiles)\n",
    "    \n",
    "    # Create 2D visualization of clusters with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X = scaled_df.drop(columns=['location', 'transient_flag', 'surge_flag', 'pf_issue_flag', 'cluster']).values\n",
    "    pca_result = pca.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        plt.scatter(\n",
    "            pca_result[cluster_labels == cluster, 0],\n",
    "            pca_result[cluster_labels == cluster, 1],\n",
    "            label=f'Cluster {cluster}', alpha=0.7, s=15\n",
    "        )\n",
    "    \n",
    "    plt.title(f'Clustering Results with {best_model}')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/clustering_{best_model}.png')\n",
    "    \n",
    "    # Map clusters to anomaly types with detailed categories\n",
    "    anomaly_mapping = {}\n",
    "    for cluster in cluster_profiles.index:\n",
    "        profile = cluster_profiles.loc[cluster]\n",
    "        \n",
    "        # Start with most severe conditions and work down\n",
    "        if profile['current'] < 0.01 and profile['power'] < 1.0:\n",
    "            anomaly_type = \"No Load (Idle Operation)\"\n",
    "        # Light load with acceptable PF\n",
    "        elif profile['current'] < 2.0 and profile['power'] < 400.0 and profile['powerFactor'] >= 0.75:\n",
    "            anomaly_type = \"Light Load (Normal Operation)\"\n",
    "        # Then continue with your existing conditions\n",
    "        elif profile['severe_voltage_dip'] > 0.3:\n",
    "            anomaly_type = \"Severe Voltage Dip\"\n",
    "        elif profile['severe_surge'] > 0.3:\n",
    "            anomaly_type = \"Severe Voltage Surge\"\n",
    "        elif profile['very_high_power'] > 0.5 and profile['powerFactor'] > 0.9:\n",
    "            anomaly_type = \"Very High Load (Good PF)\"\n",
    "        elif profile['very_high_power'] > 0.5 and profile['powerFactor'] < 0.75:\n",
    "            anomaly_type = \"Very High Load with PF Issues\"\n",
    "        elif profile['high_current'] > 0.5 and profile['moderate_pf_issue'] > 0.5:\n",
    "            anomaly_type = \"High Current with PF Issues\"\n",
    "        elif profile['moderate_voltage_dip'] > 0.3:\n",
    "            anomaly_type = \"Moderate Voltage Dip\"\n",
    "        elif profile['moderate_surge'] > 0.3:\n",
    "            anomaly_type = \"Moderate Voltage Surge\"\n",
    "        elif profile['severe_pf_issue'] > 0.3:\n",
    "            anomaly_type = \"Severe Power Factor Issue\"\n",
    "        elif profile['moderate_pf_issue'] > 0.5:\n",
    "            anomaly_type = \"Moderate Power Factor Issue\"\n",
    "        elif profile['mild_pf_issue'] > 0.5:\n",
    "            anomaly_type = \"Mild Power Factor Issue\"\n",
    "        elif profile['freq_low'] > 0.3:\n",
    "            anomaly_type = \"Low Frequency\"\n",
    "        elif profile['freq_high'] > 0.3:\n",
    "            anomaly_type = \"High Frequency\"\n",
    "        elif (profile['power'] > 1000 and profile['current'] > 5):\n",
    "            anomaly_type = \"High Load Operation\"\n",
    "        elif profile['pf_deviation'] > 0.1:\n",
    "            anomaly_type = \"Minor Power Factor Deviation\"\n",
    "        else:\n",
    "            anomaly_type = \"Normal Operation\"\n",
    "        \n",
    "        anomaly_mapping[cluster] = anomaly_type\n",
    "    \n",
    "    print(\"\\nCluster to anomaly type mapping:\")\n",
    "    for cluster, anomaly_type in anomaly_mapping.items():\n",
    "        print(f\"  Cluster {cluster} â†’ {anomaly_type}\")\n",
    "    \n",
    "    # Save results for further analysis\n",
    "    df_with_clusters['anomaly_type'] = df_with_clusters['cluster'].map(anomaly_mapping)\n",
    "    df_with_clusters.to_csv('clustered_anomalies.csv', index=False)\n",
    "    \n",
    "    return df_with_clusters, anomaly_mapping\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Power Quality Anomaly Analysis ===\")\n",
    "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # 1. Load data\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 1/8] Loading anomalous data...\")\n",
    "    df = load_anomalous_data()\n",
    "    print(f\"Step 1 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # 2. Preprocess data\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 2/8] Preprocessing data...\")\n",
    "    df = preprocess_data(df)\n",
    "    print(f\"Step 2 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # 3. Engineer features\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 3/8] Engineering features...\")\n",
    "    df = engineer_features(df)\n",
    "    print(f\"Step 3 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # 4. Reduce data while preserving patterns\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 4/8] Reducing data...\")\n",
    "    df_reduced = reduce_data(df, max_samples=20000)  # Changed to 20,000 as per your suggestion\n",
    "    print(f\"Step 4 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # 5. Normalize data\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 5/8] Normalizing data...\")\n",
    "    scaled_df, feature_cols, scaler = normalize_data(df_reduced)\n",
    "    print(f\"Step 5 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # # 6. Visualize data\n",
    "    # step_start = time.time()\n",
    "    # print(\"\\n[Step 6/8] Creating visualizations...\")\n",
    "    # pca_result = visualize_data(df_reduced, scaled_df, feature_cols)\n",
    "    # print(f\"Step 6 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # 7. Perform clustering and evaluate\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 7/8] Performing clustering analysis...\")\n",
    "    results, metrics_df, best_model = perform_clustering(scaled_df, feature_cols)\n",
    "    print(f\"Step 7 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    # 8. Analyze clusters\n",
    "    step_start = time.time()\n",
    "    print(\"\\n[Step 8/8] Analyzing clusters...\")\n",
    "    df_with_clusters, anomaly_mapping = analyze_clusters(df_reduced, scaled_df, results, best_model)\n",
    "    print(f\"Step 8 completed in {time.time() - step_start:.1f} seconds\")\n",
    "    \n",
    "    overall_time = time.time() - overall_start\n",
    "    hours = int(overall_time // 3600)\n",
    "    minutes = int((overall_time % 3600) // 60)\n",
    "    seconds = int(overall_time % 60)\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    print(f\"Total execution time: {hours}h {minutes}m {seconds}s\")\n",
    "    print(f\"Results saved to 'clustered_anomalies.csv'\")\n",
    "    print(f\"Visualizations saved to 'plots/' directory\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
