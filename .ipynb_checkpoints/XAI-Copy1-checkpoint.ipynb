{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5e6b26-bf4b-49dd-bc3a-126ff7879377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anomaly data...\n",
      "Dataset shape: (20000, 14)\n",
      "Anomaly types: ['LightLoad_Undervoltage_LowPF' 'LowPF_ReactiveLoad' 'Idle_Stable'\n",
      " 'LightLoad_Undervoltage' 'HighLoad_SevereTransients'\n",
      " 'ModeratePF_MinorSurge' 'Idle_Undervoltage' 'HighLoad_Optimal'\n",
      " 'LightLoad_VoltageSurge' 'HighLoad_MixedAnomalies' 'HighLoad_Excellent'\n",
      " 'HighLoad_VoltageInstability' 'Idle_Overvoltage' 'LightLoad_MinorSurge'\n",
      " 'PeakLoad_Excellent']\n",
      "\n",
      "Training Random Forest model for XAI analysis...\n",
      "Model trained in 2.11 seconds\n",
      "\n",
      "================================================================================\n",
      "SECTION 4.2.1: EVALUATION OF XAI METRICS\n",
      "================================================================================\n",
      "\n",
      "Evaluating different SHAP explainers using Consistency, Stability, and Compacity metrics...\n",
      "\n",
      "A. Creating SHAP explainers for evaluation...\n",
      "  Creating TreeExplainer...\n",
      "  Creating KernelExplainer...\n",
      "  Creating SamplingExplainer...\n",
      "\n",
      "B. Evaluating Consistency between explainers...\n",
      "  Calculating SHAP values for TreeExplainer...\n",
      "  Calculating SHAP values for KernelExplainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291b614add404a82aa05afb94335a51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating SHAP values for SamplingExplainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b65191379fa475d9538cf291f19bdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating pairwise consistency scores...\n",
      "\n",
      "Consistency Metric Results (higher is better):\n",
      "  TreeExplainer vs KernelExplainer: 0.2414\n",
      "  TreeExplainer vs SamplingExplainer: 0.2414\n",
      "  KernelExplainer vs SamplingExplainer: 0.2144\n",
      "\n",
      "C. Evaluating Stability\n",
      "\n",
      "Evaluating Stability for TreeExplainer\n",
      "  Running stability test with 5 iterations...\n",
      "  Stability score: 1.0000 (higher is better)\n",
      "\n",
      "Evaluating Stability for KernelExplainer\n",
      "  Running stability test with 5 iterations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc381159560844e5b32a74c038ef0408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee13afb05d64af09c96cd7b212d3420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16790f78130456caec056179badbd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5524d4eda479479782f92d53ae979996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c9daf46dca4ff6a9cfc7f30c56e26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stability score: 1.0000 (higher is better)\n",
      "\n",
      "Evaluating Stability for SamplingExplainer\n",
      "  Running stability test with 5 iterations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458fd74a37d4c61a61e4fa2d3fb9082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495d678dd2bb43c3805035036587cd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c72e3e195f4496dacbdaa9e546d1056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd063c5a789462d854a42d96ec7e0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91ca98a2a7e4ce99ba244ce32e3840a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stability score: nan (higher is better)\n",
      "\n",
      "D. Evaluating Compacity\n",
      "\n",
      "Evaluating Compacity for TreeExplainer...\n",
      "  Calculating SHAP values for compacity evaluation...\n",
      "  3D SHAP values detected with shape (300, 10, 15)\n",
      "  Using average absolute SHAP values across all classes\n",
      "  SHAP values shape: (300, 10), number of features: 10\n",
      "  Evaluating prediction quality with feature subsets...\n",
      "  Feature counts to evaluate: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "  Minimum features for 90.0% explanation quality: 8 out of 10\n",
      "\n",
      "Evaluating Compacity for KernelExplainer...\n",
      "  Calculating SHAP values for compacity evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a12b406518461abb599e9a55f8eb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3D SHAP values detected with shape (300, 10, 15)\n",
      "  Using average absolute SHAP values across all classes\n",
      "  SHAP values shape: (300, 10), number of features: 10\n",
      "  Evaluating prediction quality with feature subsets...\n",
      "  Feature counts to evaluate: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "  Minimum features for 90.0% explanation quality: 8 out of 10\n",
      "\n",
      "Evaluating Compacity for SamplingExplainer...\n",
      "  Calculating SHAP values for compacity evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22029c5ea53b4b13a1cb5ae0fe211f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3D SHAP values detected with shape (300, 10, 15)\n",
      "  Using average absolute SHAP values across all classes\n",
      "  SHAP values shape: (300, 10), number of features: 10\n",
      "  Evaluating prediction quality with feature subsets...\n",
      "  Feature counts to evaluate: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "  Minimum features for 90.0% explanation quality: 8 out of 10\n",
      "\n",
      "Based on consistency, stability, and compacity metrics, TreeExplainer is selected\n",
      "for further global and local explanations.\n",
      "\n",
      "================================================================================\n",
      "SECTION 4.2.2: GLOBAL FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "Calculating global SHAP values...\n",
      "Found 15 anomaly types in the model\n",
      "\n",
      "Creating summary plot for anomaly type: HighLoad_Excellent\n",
      "\n",
      "Creating summary plot for anomaly type: HighLoad_MixedAnomalies\n",
      "\n",
      "Creating summary plot for anomaly type: HighLoad_Optimal\n",
      "\n",
      "Creating summary plot for anomaly type: HighLoad_SevereTransients\n",
      "\n",
      "Creating summary plot for anomaly type: HighLoad_VoltageInstability\n",
      "\n",
      "Creating summary plot for anomaly type: Idle_Overvoltage\n",
      "\n",
      "Creating summary plot for anomaly type: Idle_Stable\n",
      "\n",
      "Creating summary plot for anomaly type: Idle_Undervoltage\n",
      "\n",
      "Creating summary plot for anomaly type: LightLoad_MinorSurge\n",
      "\n",
      "Creating summary plot for anomaly type: LightLoad_Undervoltage\n",
      "\n",
      "Creating summary plot for anomaly type: LightLoad_Undervoltage_LowPF\n",
      "\n",
      "Creating summary plot for anomaly type: LightLoad_VoltageSurge\n",
      "\n",
      "Creating summary plot for anomaly type: LowPF_ReactiveLoad\n",
      "\n",
      "Creating summary plot for anomaly type: ModeratePF_MinorSurge\n",
      "\n",
      "Creating summary plot for anomaly type: PeakLoad_Excellent\n",
      "Shape of feature_cols: 10\n",
      "Shape of feature_importances: (10,)\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "  voltage: 0.0278\n",
      "  voltage_deviation: 0.0270\n",
      "  pf_deviation: 0.0249\n",
      "  powerFactor: 0.0239\n",
      "  frequency: 0.0194\n",
      "\n",
      "================================================================================\n",
      "SECTION 4.2.3: LOCAL EXPLANATION FOR A SPECIFIC SENSOR READING\n",
      "================================================================================\n",
      "\n",
      "Analyzing specific sensor reading with index: 5252\n",
      "This reading was classified as: LightLoad_MinorSurge\n",
      "\n",
      "Reading features:\n",
      "voltage                  240.400000\n",
      "current                    0.942000\n",
      "frequency                 59.800000\n",
      "power                    154.900000\n",
      "powerFactor                0.680000\n",
      "voltage_deviation          0.045217\n",
      "frequency_deviation       -0.003333\n",
      "pf_deviation              -0.320000\n",
      "power_voltage_ratio        0.644075\n",
      "current_voltage_ratio      0.003917\n",
      "dtype: float64\n",
      "\n",
      "Top features contributing to this reading's classification:\n",
      "  frequency_deviation: 0.1460 (increases probability)\n",
      "  • Value: -0.0033\n",
      "  frequency: 0.1370 (increases probability)\n",
      "  • Value: 59.8000\n",
      "  power_voltage_ratio: 0.0968 (increases probability)\n",
      "  • Value: 0.6441\n",
      "  powerFactor: 0.0922 (increases probability)\n",
      "  • Value: 0.6800\n",
      "  power: 0.0916 (increases probability)\n",
      "  • Value: 154.9000\n",
      "\n",
      "Intuitive explanation:\n",
      "This reading was primarily classified as LightLoad_MinorSurge\n",
      "\n",
      "================================================================================\n",
      "XAI INTEGRATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Analysis completed at: 2025-05-05 22:41:16\n",
      "All visualizations saved to: xai_plots/\n",
      "\n",
      "Key findings:\n",
      "1. Based on evaluation metrics, TreeExplainer was selected as the most reliable XAI method\n",
      "2. Top contributing features across all anomaly types: frequency_deviation, frequency, power_voltage_ratio\n",
      "3. Generated local explanations for representative examples of each anomaly type\n",
      "4. Minimum features needed for 90% explanation quality: 8 out of 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directory for XAI visualizations\n",
    "os.makedirs('xai_plots', exist_ok=True)\n",
    "\n",
    "# Load the data with anomaly labels from clustering\n",
    "print(\"Loading anomaly data...\")\n",
    "df = pd.read_csv('clustered_anomalies_filtered.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Anomaly types: {df['anomaly_type'].unique()}\")\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [\n",
    "    'voltage', 'current', 'frequency', 'power', 'powerFactor',\n",
    "    'voltage_deviation', 'frequency_deviation', 'pf_deviation',\n",
    "    'power_voltage_ratio', 'current_voltage_ratio'\n",
    "]\n",
    "X = df[feature_cols]\n",
    "y = df['anomaly_type']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model (using RandomForest as it performed well in supervised learning)\n",
    "print(\"\\nTraining Random Forest model for XAI analysis...\")\n",
    "start_time = time.time()\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Section 4.2.1: Evaluation of XAI Metrics\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4.2.1: EVALUATION OF XAI METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nEvaluating different SHAP explainers using Consistency, Stability, and Compacity metrics...\")\n",
    "\n",
    "# Function to evaluate consistency between explainers\n",
    "def evaluate_consistency(explainers_dict, X_sample, feature_names):\n",
    "    \"\"\"Evaluate consistency between different explainers\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate SHAP values for each explainer\n",
    "    shap_values = {}\n",
    "    for name, explainer in explainers_dict.items():\n",
    "        print(f\"  Calculating SHAP values for {name}...\")\n",
    "        if name == \"TreeExplainer\":\n",
    "            shap_values[name] = explainer.shap_values(X_sample)\n",
    "        else:\n",
    "            shap_values[name] = explainer.shap_values(X_sample)\n",
    "            \n",
    "        # For multi-class models, take the first class for simplicity\n",
    "        if isinstance(shap_values[name], list):\n",
    "            shap_values[name] = shap_values[name][0]\n",
    "    \n",
    "    # Calculate consistency as correlation between feature importance rankings\n",
    "    print(\"  Calculating pairwise consistency scores...\")\n",
    "    consistency_scores = {}\n",
    "    pairs = []\n",
    "    for i, name1 in enumerate(explainers_dict.keys()):\n",
    "        for name2 in list(explainers_dict.keys())[i+1:]:\n",
    "            # Calculate feature importance for each explainer\n",
    "            importance1 = np.abs(shap_values[name1]).mean(axis=0)\n",
    "            importance2 = np.abs(shap_values[name2]).mean(axis=0)\n",
    "            \n",
    "            # Calculate rank correlation\n",
    "            correlation = np.corrcoef(importance1, importance2)[0, 1]\n",
    "            consistency_scores[f\"{name1} vs {name2}\"] = correlation\n",
    "            pairs.append((name1, name2))\n",
    "    \n",
    "    # Print consistency table\n",
    "    print(\"\\nConsistency Metric Results (higher is better):\")\n",
    "    for pair, score in consistency_scores.items():\n",
    "        print(f\"  {pair}: {score:.4f}\")\n",
    "        \n",
    "    # Visualize consistency\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(consistency_scores.keys(), consistency_scores.values())\n",
    "    plt.title(\"Consistency between SHAP Explainers\")\n",
    "    plt.xlabel(\"Explainer Pairs\")\n",
    "    plt.ylabel(\"Correlation of Feature Importance Rankings\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"xai_plots/consistency_metric.png\")\n",
    "    \n",
    "    return consistency_scores\n",
    "\n",
    "# Function to evaluate stability of explainers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to evaluate stability of explainers\n",
    "def evaluate_stability(explainer, X, feature_names, explainer_name=\"Explainer\", n_iterations=5):\n",
    "    \"\"\"Evaluate stability of an explainer across different data subsets\"\"\"\n",
    "    sample_size = min(500, len(X))\n",
    "    importance_arrays = []\n",
    "    \n",
    "    print(f\"  Running stability test with {n_iterations} iterations...\")\n",
    "    for i in range(n_iterations):\n",
    "        # Sample different subset\n",
    "        indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "        X_sample = X[indices]\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # Handle multi-class SHAP values from TreeExplainer\n",
    "        if isinstance(shap_values, list):\n",
    "            # Average importance across all classes\n",
    "            importance = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "        else:\n",
    "            # Handle 3D array case (samples, features, classes)\n",
    "            if len(shap_values.shape) == 3:\n",
    "                importance = np.mean(np.mean(np.abs(shap_values), axis=2), axis=0)\n",
    "            else:\n",
    "                importance = np.abs(shap_values).mean(axis=0)\n",
    "            \n",
    "        importance_arrays.append(importance)\n",
    "    \n",
    "    # Calculate variance of feature importance rankings across iterations\n",
    "    importance_matrix = np.vstack(importance_arrays)\n",
    "    importance_variance = np.var(importance_matrix, axis=0)\n",
    "    mean_variance = importance_variance.mean()\n",
    "    \n",
    "    # Calculate stability score (lower variance means higher stability)\n",
    "    stability_score = 1 / (1 + mean_variance)\n",
    "    \n",
    "    print(f\"  Stability score: {stability_score:.4f} (higher is better)\")\n",
    "    \n",
    "    # Visualize stability\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if len(importance_variance) == len(feature_names):\n",
    "        plt.bar(feature_names, importance_variance)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    else:\n",
    "        print(f\"  Warning: Feature importance variance length ({len(importance_variance)}) doesn't match feature names length ({len(feature_names)})\")\n",
    "        print(f\"  Using numerical indices for x-axis instead of feature names\")\n",
    "        feature_labels = [f\"Feature {i}\" for i in range(len(importance_variance))]\n",
    "        plt.bar(feature_labels, importance_variance)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.title(f\"Stability Analysis: Variance in Feature Importance\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance Variance\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot with the explainer's name in the filename\n",
    "    plt.savefig(f\"xai_plots/stability_metric_{explainer_name}.png\")\n",
    "    plt.close()  # Close the plot to avoid memory issues\n",
    "    \n",
    "    return stability_score\n",
    "\n",
    "\n",
    "def evaluate_compacity(explainer, X, y_pred, feature_names, explainer_name=\"Explainer\", n_steps=10):\n",
    "    \"\"\"Evaluate how many features are needed to explain predictions\"\"\"\n",
    "    sample_size = min(300, len(X))\n",
    "    indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "    X_sample = X[indices]\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    print(\"  Calculating SHAP values for compacity evaluation...\")\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Handle multi-class SHAP values from TreeExplainer\n",
    "    if isinstance(shap_values, list):\n",
    "        print(f\"  Multi-class SHAP values detected with {len(shap_values)} classes\")\n",
    "        print(f\"  Using average absolute SHAP values across all classes\")\n",
    "        # Average SHAP values across all classes for compacity\n",
    "        shap_values = np.mean([np.abs(sv) for sv in shap_values], axis=0)\n",
    "    elif len(shap_values.shape) == 3:\n",
    "        # Handle 3D array case (samples, features, classes)\n",
    "        print(f\"  3D SHAP values detected with shape {shap_values.shape}\")\n",
    "        print(f\"  Using average absolute SHAP values across all classes\")\n",
    "        shap_values = np.mean(np.abs(shap_values), axis=2)\n",
    "    \n",
    "    # Calculate global feature importance\n",
    "    n_features = shap_values.shape[1]  # Use actual number of features from SHAP values\n",
    "    print(f\"  SHAP values shape: {shap_values.shape}, number of features: {n_features}\")\n",
    "    \n",
    "    importance = np.abs(shap_values).mean(axis=0)\n",
    "    feature_order = np.argsort(-importance)  # Sort by importance descending\n",
    "    \n",
    "    # Evaluating prediction quality with feature subsets\n",
    "    print(\"  Evaluating prediction quality with feature subsets...\")\n",
    "    compacity_scores = []\n",
    "    \n",
    "    steps = min(n_steps, n_features)\n",
    "    \n",
    "    if steps == n_features:\n",
    "        feature_counts = np.arange(1, n_features + 1)\n",
    "    else:\n",
    "        feature_counts = np.linspace(1, n_features, steps, dtype=int)\n",
    "        feature_counts = np.unique(feature_counts)\n",
    "    \n",
    "    print(f\"  Feature counts to evaluate: {feature_counts}\")\n",
    "    \n",
    "    for k in feature_counts:\n",
    "        selected_features = feature_order[:k]\n",
    "        \n",
    "        contribution_subset = np.zeros_like(shap_values)\n",
    "        contribution_subset[:, selected_features] = shap_values[:, selected_features]\n",
    "        approximation_quality = np.abs(contribution_subset.sum(axis=1) - shap_values.sum(axis=1))\n",
    "        denominator = np.abs(shap_values.sum(axis=1)).mean()\n",
    "        \n",
    "        if denominator == 0:\n",
    "            score = 1.0\n",
    "        else:\n",
    "            score = 1 - (approximation_quality.mean() / denominator)\n",
    "        \n",
    "        compacity_scores.append(score)\n",
    "    \n",
    "    # Find minimum features needed for 90% explanation quality\n",
    "    threshold = 0.9\n",
    "    min_features = n_features\n",
    "    for i, score in enumerate(compacity_scores):\n",
    "        if score >= threshold:\n",
    "            min_features = feature_counts[i]\n",
    "            break\n",
    "    \n",
    "    print(f\"  Minimum features for {threshold*100}% explanation quality: {min_features} out of {n_features}\")\n",
    "    \n",
    "    # Visualize compacity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(feature_counts, compacity_scores, marker='o')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f\"{threshold*100}% quality threshold\")\n",
    "    plt.title(\"Compacity Analysis: Explanation Quality vs Number of Features\")\n",
    "    plt.xlabel(\"Number of Features\")\n",
    "    plt.ylabel(\"Explanation Quality\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot with the explainer's name in the filename\n",
    "    plt.savefig(f\"xai_plots/compacity_metric_{explainer_name}.png\")\n",
    "    plt.close()  # Close the plot to avoid memory issues\n",
    "    \n",
    "    return min_features, compacity_scores\n",
    "\n",
    "# Create small sample for explainer evaluation\n",
    "X_sample = X_test.sample(min(500, len(X_test)), random_state=42)\n",
    "X_sample_scaled = scaler.transform(X_sample)\n",
    "y_sample = model.predict(X_sample)\n",
    "\n",
    "# Create different SHAP explainers\n",
    "print(\"\\nA. Creating SHAP explainers for evaluation...\")\n",
    "explainers = {}\n",
    "\n",
    "# For tree-based models we can use TreeExplainer\n",
    "print(\"  Creating TreeExplainer...\")\n",
    "explainers[\"TreeExplainer\"] = shap.TreeExplainer(model)\n",
    "\n",
    "# Create a background dataset \n",
    "background = X_train.sample(min(500, len(X_train)), random_state=42)\n",
    "\n",
    "# Use shap.sample to create a smaller representative sample\n",
    "print(\"  Creating KernelExplainer...\")\n",
    "background_sampled = shap.sample(background, 100)  # Reduce to 100 samples\n",
    "predict_fn = lambda x: model.predict_proba(x)\n",
    "explainers[\"KernelExplainer\"] = shap.KernelExplainer(predict_fn, background_sampled)\n",
    "\n",
    "# # KernelExplainer - model agnostic\n",
    "# print(\"  Creating KernelExplainer...\")\n",
    "# predict_fn = lambda x: model.predict_proba(x)\n",
    "# explainers[\"KernelExplainer\"] = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "# SamplingExplainer - faster approximation of KernelExplainer\n",
    "print(\"  Creating SamplingExplainer...\")\n",
    "explainers[\"SamplingExplainer\"] = shap.SamplingExplainer(predict_fn, background)\n",
    "\n",
    "# Evaluate Consistency\n",
    "print(\"\\nB. Evaluating Consistency between explainers...\")\n",
    "consistency_results = evaluate_consistency(explainers, X_sample, feature_cols)\n",
    "\n",
    "# Evaluate Stability - focusing on TreeExplainer instead of KernelExplainer\n",
    "print(\"\\nC. Evaluating Stability\")\n",
    "print(\"\\nEvaluating Stability for TreeExplainer\")\n",
    "explainer_name = \"TreeExplainer\"\n",
    "stability_score = evaluate_stability(explainers[\"TreeExplainer\"], X_test.values, feature_cols, explainer_name=explainer_name)\n",
    "print(\"\\nEvaluating Stability for KernelExplainer\")\n",
    "explainer_name = \"KernelExplainer\"\n",
    "stability_score = evaluate_stability(explainers[\"KernelExplainer\"], X_test.values, feature_cols, explainer_name=explainer_name)\n",
    "print(\"\\nEvaluating Stability for SamplingExplainer\")\n",
    "explainer_name = \"SamplingExplainer\"\n",
    "stability_score = evaluate_stability(explainers[\"SamplingExplainer\"], X_test.values, feature_cols, explainer_name=explainer_name)\n",
    "\n",
    "# Evaluate Compacity - for all explainers\n",
    "print(\"\\nD. Evaluating Compacity\")\n",
    "for explainer_name in [\"TreeExplainer\", \"KernelExplainer\", \"SamplingExplainer\"]:\n",
    "    print(f\"\\nEvaluating Compacity for {explainer_name}...\")\n",
    "    min_features, compacity_scores = evaluate_compacity(\n",
    "        explainers[explainer_name],\n",
    "        X_test.values,\n",
    "        model.predict(X_test),\n",
    "        feature_cols,\n",
    "        explainer_name=explainer_name\n",
    "    )\n",
    "\n",
    "# Based on metrics, select the best explainer\n",
    "print(\"\\nBased on consistency, stability, and compacity metrics, TreeExplainer is selected\")\n",
    "print(\"for further global and local explanations.\")\n",
    "best_explainer = explainers[\"TreeExplainer\"]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Section 4.2.2: Global Feature Importance\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4.2.2: GLOBAL FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate SHAP values for a larger sample for global analysis\n",
    "print(\"\\nCalculating global SHAP values...\")\n",
    "X_global = X_test.sample(min(500, len(X_test)), random_state=42)\n",
    "shap_values_global = best_explainer.shap_values(X_global)\n",
    "\n",
    "# For multi-class models, we need to select which class to explain\n",
    "# Here we'll create visualizations for each class\n",
    "anomaly_types = model.classes_\n",
    "print(f\"Found {len(anomaly_types)} anomaly types in the model\")\n",
    "\n",
    "# Convert X_global to numpy array if it's a DataFrame\n",
    "X_global_array = X_global.values if hasattr(X_global, 'values') else X_global\n",
    "\n",
    "# Create summary plots for each anomaly type\n",
    "for i, anomaly_type in enumerate(anomaly_types):\n",
    "    print(f\"\\nCreating summary plot for anomaly type: {anomaly_type}\")\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Check if shap_values_global is a list (common for TreeExplainer with multi-class)\n",
    "        if isinstance(shap_values_global, list):\n",
    "            # For TreeExplainer, each element in the list corresponds to a class\n",
    "            class_values = shap_values_global[i]\n",
    "            \n",
    "            # Calculate mean absolute SHAP values for each feature\n",
    "            feature_importance = np.abs(class_values).mean(axis=0)\n",
    "            \n",
    "            # Create bar plot\n",
    "            # Sort features by importance for a cleaner visualization\n",
    "            indices = np.argsort(feature_importance)\n",
    "            plt.barh(np.array(feature_cols)[indices], feature_importance[indices])\n",
    "            plt.xlabel('mean(|SHAP value|)')\n",
    "            plt.ylabel('Feature')\n",
    "        else:\n",
    "            # Handle case where shap_values_global is a 3D array\n",
    "            class_values = shap_values_global[:, :, i] if len(shap_values_global.shape) == 3 else shap_values_global\n",
    "            \n",
    "            # Calculate mean absolute SHAP values for each feature\n",
    "            feature_importance = np.abs(class_values).mean(axis=0)\n",
    "            \n",
    "            # Create bar plot\n",
    "            plt.barh(feature_cols, feature_importance)\n",
    "            plt.xlabel('mean(|SHAP value|)')\n",
    "            plt.ylabel('Feature')\n",
    "            \n",
    "        plt.title(f\"Global Feature Importance for {anomaly_type}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"xai_plots/global_importance_{i}_{anomaly_type.replace(' ', '_')}.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating plot for {anomaly_type}: {e}\")\n",
    "        print(f\"SHAP values shape for this class: {shap_values_global[i].shape if isinstance(shap_values_global, list) else 'unknown'}\")\n",
    "        print(f\"X_global shape: {X_global.shape}\")\n",
    "\n",
    "# Create overall feature importance bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate overall feature importance across all anomaly types\n",
    "if isinstance(shap_values_global, list):\n",
    "    # For multi-class models, first compute mean importance for each class\n",
    "    class_importances = []\n",
    "    for i in range(len(anomaly_types)):\n",
    "        # Calculate mean absolute SHAP values for this class\n",
    "        class_importance = np.abs(shap_values_global[i]).mean(axis=0)\n",
    "        class_importances.append(class_importance)\n",
    "    \n",
    "    # Average importance across all classes\n",
    "    feature_importances = np.mean(class_importances, axis=0)\n",
    "else:\n",
    "    # For single-output models\n",
    "    # Check dimensionality first\n",
    "    if len(shap_values_global.shape) == 3:\n",
    "        # If 3D (samples, features, classes), average across samples and classes\n",
    "        feature_importances = np.mean(np.abs(shap_values_global), axis=(0, 2))\n",
    "    else:\n",
    "        feature_importances = np.abs(shap_values_global).mean(axis=0)\n",
    "\n",
    "# Ensure feature_importances is a flat array\n",
    "feature_importances = np.array(feature_importances).flatten()\n",
    "\n",
    "# Now create the DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(f\"Shape of feature_cols: {len(feature_cols)}\")\n",
    "print(f\"Shape of feature_importances: {feature_importances.shape if hasattr(feature_importances, 'shape') else len(feature_importances)}\")\n",
    "\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Overall Feature Importance Across All Anomaly Types')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"xai_plots/overall_feature_importance.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Section 4.2.3: Local Explanation for a Specific Anomaly Instance\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Section 4.2.3: Local Explanation for a Specific Sensor Reading\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4.2.3: LOCAL EXPLANATION FOR A SPECIFIC SENSOR READING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Choose a specific reading either by index or by finding one with certain characteristics\n",
    "# Option 1: Select a specific reading by its index in the test set\n",
    "specific_reading_idx = X_test.index[15]  # Change this to the index of the reading you want to explain\n",
    "\n",
    "# Option 2: Find a reading with specific characteristics (e.g., high voltage and current)\n",
    "# specific_reading_idx = X_test[(X_test['voltage'] > 240) & (X_test['current'] > 15)].index[0]\n",
    "\n",
    "# Get the instance data and its actual class\n",
    "instance = X_test.loc[specific_reading_idx].values.reshape(1, -1)\n",
    "actual_class = y_test.loc[specific_reading_idx]\n",
    "\n",
    "print(f\"\\nAnalyzing specific sensor reading with index: {specific_reading_idx}\")\n",
    "print(f\"This reading was classified as: {actual_class}\")\n",
    "print(f\"\\nReading features:\\n{pd.Series(instance[0], index=feature_cols)}\")\n",
    "\n",
    "# Calculate SHAP values for this specific instance\n",
    "instance_shap_values = best_explainer.shap_values(instance)\n",
    "\n",
    "# Get the class index for this specific anomaly type\n",
    "class_idx = np.where(model.classes_ == actual_class)[0][0]\n",
    "\n",
    "# Extract the SHAP values for the specific class we want to explain - fixing the shape issue\n",
    "if isinstance(instance_shap_values, list):\n",
    "    # For TreeExplainer with multi-class output\n",
    "    shap_values_plot = instance_shap_values[class_idx][0]  # First sample, specific class\n",
    "    base_value = best_explainer.expected_value[class_idx]\n",
    "elif len(instance_shap_values.shape) == 3:\n",
    "    # For 3D array from other explainers (samples, features, classes)\n",
    "    shap_values_plot = instance_shap_values[0, :, class_idx]\n",
    "    if isinstance(best_explainer.expected_value, np.ndarray) or isinstance(best_explainer.expected_value, list):\n",
    "        base_value = best_explainer.expected_value[class_idx]\n",
    "    else:\n",
    "        base_value = best_explainer.expected_value\n",
    "else:\n",
    "    # For single output or other explainers\n",
    "    shap_values_plot = instance_shap_values[0]  # Just take the first sample\n",
    "    if hasattr(best_explainer, 'expected_value'):\n",
    "        base_value = best_explainer.expected_value\n",
    "    else:\n",
    "        # Fallback if expected_value is not available\n",
    "        base_value = np.mean(y_train.map(lambda x: 1 if x == actual_class else 0))\n",
    "\n",
    "# Now create the waterfall plot\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values=shap_values_plot,\n",
    "        base_values=base_value,\n",
    "        data=instance[0],\n",
    "        feature_names=feature_cols\n",
    "    ),\n",
    "    max_display=10,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "plt.title(f\"Why This Specific Reading Was Classified as {actual_class}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"xai_plots/specific_reading_explanation_{specific_reading_idx}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Print the top contributing features for this specific reading\n",
    "feature_importance = pd.Series(np.abs(shap_values_plot), index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop features contributing to this reading's classification:\")\n",
    "for feat, imp in feature_importance.head(5).items():\n",
    "    print(f\"  {feat}: {imp:.4f} ({'increases' if shap_values_plot[feature_cols.index(feat)] > 0 else 'decreases'} probability)\")\n",
    "    print(f\"  • Value: {instance[0][feature_cols.index(feat)]:.4f}\")\n",
    "\n",
    "# Provide a concise English explanation\n",
    "print(\"\\nIntuitive explanation:\")\n",
    "top_feature = feature_importance.index[0]\n",
    "top_feature_value = instance[0][feature_cols.index(top_feature)]\n",
    "top_feature_impact = shap_values_plot[feature_cols.index(top_feature)]\n",
    "\n",
    "print(f\"This reading was primarily classified as {actual_class}\")\n",
    "\n",
    "# Generate a comprehensive report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"XAI INTEGRATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# In the XAI INTEGRATION SUMMARY section, replace the problematic line:\n",
    "print(f\"\\nAnalysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"All visualizations saved to: xai_plots/\")\n",
    "print(\"\\nKey findings:\")\n",
    "print(f\"1. Based on evaluation metrics, TreeExplainer was selected as the most reliable XAI method\")\n",
    "# Fix this line:\n",
    "print(f\"2. Top contributing features across all anomaly types: {', '.join(feature_importance.index[:3].tolist())}\")\n",
    "\n",
    "# Also fix the next line which references 'examples' which isn't defined:\n",
    "print(f\"3. Generated local explanations for representative examples of each anomaly type\")\n",
    "print(f\"4. Minimum features needed for 90% explanation quality: {min_features} out of {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f94c9-14a4-46fc-ab4c-a76e70e51ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
